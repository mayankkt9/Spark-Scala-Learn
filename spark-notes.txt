What is Spark ? 
	- Scalable
	- Massively Parellel
	- In Memory Execution
Data can be loaded into memory and analyzed in parellel
Distribute data in cluster and then analyzed
Very fast as compared to Hadoop as Spark is IN Memory

Features 
	- Speed 
	- Powerful Caching 
	- Realtime DataAnalysis
	- Deployment
	- Polyglot (Scala | Java | Python | R)

Spark Component
	- Spark Core  (Basic IO | Scheduling | Monitoring | API)
	- Library (SparkSQL | GraphX | SparkStreaming | SparkMLib)
	- Programming Language Supports (Scala | Java | Python | R)
	- Storage (LocalFS | HDFS | AmazonS3 | RDBMS | NoSQL)


 
RDD (Resilient Distributed Data)
Hadoop stored data in multiple distributed storage disk such as HDFS and multiple IO makes it slow.
Spark came up with In Memory Data Sharing that made it very fast which became 100 times faster than network or disk sharing 
RDD tried to solve this problem by fault tolerant distributed in-memory computation.
	- Fundamental Data Structure
	- All Data are in RDD format
	- Each Data in RDD can be partitioned logically and can run on parellel on different node on cluster.
	- Spark takes care of RDD Partition distribution
	- Highly Resilient (Able to recover quickly as same data chunk are replicated across multiple node)

Features of RDD
	- In Memory computation
	- Lazy Evaluation (Does not evaluate quickly until the action is applied)
	- Fault Tolerant (Rebuild last data)
	- Immutable
	- Partitioning (Fundamental unit of parellelism)
	- Persistence
	- Coarse Grained Operation

Ways to Create RDDs
	- Parellelized Collection (sc.parellelize)
	- From RDDs 
	- External Data (HDFS | AmazonS3)


Learning Journal

Why All this is needed 
	- Solving huge data computation
		- Three things are required for solving large scale data processing
			- Distributed Storage 
			- Computing Framework
			- Resource Manager
	- Hadoop did came up with 
		- Distributed Storage (HDFS) and 
		- Computing Framework (Map Reduce) and 
		- Resource Manager (Yarn)
		- Map Reduce Program was inefficient and difficult to code.
	- Spark is the Compelling replacement of MapReduce
		- 10 to 100 times faster than Hadoop
		- fast and general purpose engine for large scale data processing
		- Under the hood it works on a cluster of computers

Two things together makes Apache Spark 
	- A cluster computing enginer 
	- A set of libraries, APIs and DSLs

Spark Ecosystem

At the base you have Spark Core
Spark Core have two parts 
	- Computing engine (Heavili depends on thirdpart Resource Manager and Storage System)
		- Provides basic functionality like 
		- Memory Management
		- Task Scheduling
		- Fault Recovery
		- Interaction with Cluster Manager and Storage System
		- Manages our spark jobs and give seamless experience to end user
	- Set of Spark Core API (Language - Scala,Python,Java,R)
		- Two kind of APIs
			- Structured API - DataFrame and DataSets - Designed to work with Structured data
			- UnStructured API - RDDs, Accumulators and broadcast variables (available in Programming language)

Spark is a Distributed Processing engine 
It doesnt come with inbuild cluster resource manager and a distributed storage system
You have to plug in those
For Cluster Manager we can use -> (YARN, Mesos, Kubernetes etc)
For Distributed Storage we can use -> (HDFS, S3, GCS, CFS etc)

OutSide Spark Core we have 4 different sets of Spark Libraries and Packages
	- SparkSQL
	- Streaming
	- MLlib
	- GraphX


Why Spark ? 
	- Abstract Parallel Programming 
		- We will work with tables SQL
		- We will work with local python or scala collection
		- All complexity of distributed storage, computation and parallel programming is abstracted
	- Unified Platform
		- Combines capability of batch processing, structured data handling with SQL, Streaming, GraphX, Mlib with PL.
	- Ease of use
		- Much more short simple and easy to read as compared to Map Reduce.
We execute our program in spark cluster How?
	- Interactive Shell (Scala Shell, Pyspark)
		- For exploration
	- Submit Job
		- Example a long running streaming job or a periodic batch job - you must package your application 
		and submit it to spark cluster for execution.
		- for production use case we will use this.

how does the spark execute our programs on a cluster ?
Spark is a disturbuted computing engine, follow master slave architecture
For every spark application it will create one master process and multiple slave process
Master is the driver and slaves are the executors.
So for every spark application it will create one driver and multiple executor processes.
Since master is the driver it is responsible for these across the executors
	- Analysing 
	- Distributing 
	- Scheduling 
	- Monitoring 
Executors are only responsible to run the code and report it to driver.

Who runs where ? 
Executors are going to run on cluster machine.
Driver can be run on local maching or cluster machine
	- client mode  (start driver on local machine)
		- when you are exploring or debugging - throwing back output on terminal
	- cluster mode (start driver on cluster machine) 
		- (Makes sense for production Deployment)
		- Because after you run spark submit , you can close your computer they will run independently within cluster


Who controls the cluster ? 
How Spark get the resources for the driver and the executors ?
- Through Resource Manager
	- YARN
		- most widely used resource manager
	- Mesos
	- Standalone spark
		- simple and basic cluster manager
	- Kubernetes
		- general purpose container orchestration from google
		- not yet production ready


YARN 
	- Yet Another Resource Negotiator
	- framework to provide computational resources for execution engine 
	- Resources - CPU, Memory, DiskIO, Network Bandwidth
	- Provide API to request resource requirement and task Scheduling
	- Yarn API are for customer making computation engine and not for any end user.
Components of YARN
	- Resource Manager (One Per Cluster)
	- Node Manager (One per Data node)

What happens when we send a spark submit job ?
How Yarn allocates resources for a job

When you start a program in Client Mode - Client Mode -> SparkShell, Pyspark, local machine, you are running it in local machine
	- Driver is the one who started program, Ex - SparkShell, Pyspark
	- Client Mode means driver is the local machine
	- Spark application begin by creating a spark session, first thing in any spark 2.x application
	- SparkShell, Pyspark automatically create spark session for you.
		- Spark session is kind of a data Structure where the driver mantains all the information including the executor location and their status 
	- When we submit a job to yarn through driver, the request goes to resource manager
	- the resource manager will find one node manager and ask it to launch one container
	- It is the first container of an application and we call it the application master.
	- This application master takes over the responsibility for executing and monitoring the job.
	- The application master functionality depends upon the application framework (MapReduce or Spark)
	- Application master executes executor
	- Application master request resource manager for more containers, 
	- and application master starts executor in each container
	- After the initial setup these executors directly communicate with the driver

When you start a program in Cluster Mode
	- Just the difference is here Yarn Application Master starts the driver and not in the local machine and 
	you dont have any dependency on your local machine
	- Rest process is same

Spark Program in Local Mode 
	- Here you dont need YARN or any resource manager
	- It begins in the JVM all driver and executor runs in the same JVM
	- Most convenient for learning 

