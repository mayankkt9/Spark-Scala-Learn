What is Spark ? 
	- Scalable
	- Massively Parellel
	- In Memory Execution
Data can be loaded into memory and analyzed in parellel
Distribute data in cluster and then analyzed
Very fast as compared to Hadoop as Spark is IN Memory

Features 
	- Speed 
	- Powerful Caching 
	- Realtime DataAnalysis
	- Deployment
	- Polyglot (Scala | Java | Python | R)

Spark Component
	- Spark Core  (Basic IO | Scheduling | Monitoring | API)
	- Library (SparkSQL | GraphX | SparkStreaming | SparkMLib)
	- Programming Language Supports (Scala | Java | Python | R)
	- Storage (LocalFS | HDFS | AmazonS3 | RDBMS | NoSQL)

 
RDD (Resilient Distributed Data)
Hadoop stored data in multiple distributed storage disk such as HDFS and multiple IO makes it slow.
Spark came up with In Memory Data Sharing that made it very fast which became 100 times faster than network or disk sharing 
RDD tried to solve this problem by fault tolerant distributed in-memory computation.
	- Fundamental Data Structure
	- All Data are in RDD format
	- Each Data in RDD can be partitioned logically and can run on parellel on different node on cluster.
	- Spark takes care of RDD Partition distribution
	- Highly Resilient (Able to recover quickly as same data chunk are replicated across multiple node)

Features of RDD
	- In Memory computation
	- Lazy Evaluation (Does not evaluate quickly until the action is applied)
	- Fault Tolerant (Rebuild last data)
	- Immutable
	- Partitioning (Fundamental unit of parellelism)
	- Persistence
	- Coarse Grained Operation

Ways to Create RDDs
	- Parellelized Collection (sc.parellelize)
	- From RDDs 
	- External Data (HDFS | AmazonS3)


Learning Journal

Why All this is needed 
	- Solving huge data computation
		- Three things are required for solving large scale data processing
			- Distributed Storage 
			- Computing Framework
			- Resource Manager
	- Hadoop did came up with 
		- Distributed Storage (HDFS) and 
		- Computing Framework (Map Reduce) and 
		- Resource Manager (Yarn)
		- Map Reduce Program was inefficient and difficult to code.
	- Spark is the Compelling replacement of MapReduce
		- 10 to 100 times faster than Hadoop
		- fast and general purpose engine for large scale data processing
		- Under the hood it works on a cluster of computers

Two things together makes Apache Spark 
	- A cluster computing enginer 
	- A set of libraries, APIs and DSLs

Spark Ecosystem

At the base you have Spark Core
Spark Core have two parts 
	- Computing engine
		- Provides basic functionality like 
		- Memory Management
		- Task Scheduling
		- Fault Recovery
		- Interaction with Cluster Manager and Storage System
		- Manages our spark jobs and give seamless experience to end user
	- Set of Spark Core API (Language - Scala,Python,Java,R)
		- Two kind of APIs
		- Structured API - DataFrame and DataSets - Designed to work with Structured data
		- UnStructured API - RDDS, Accumulators and broadcast variables (available in Programming language)

Spark is a Distributed Processing engine 
It doesnt come with inbuild cluster resource manager and a distributed storage system
You have to plug in those
For Cluster Manager we can use -> (YARN, Mesos, Kubernetes etc)
For Distributed Storage we can use -> (HDFS, S3, GCS, CFS etc)

OutSide Spark Core we have 4 different sets of Spark Libraries and Packages
	- SparkSQL
	- Streaming
	- MLlib
	- GraphX


